{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNP9ADpBw7LkYwTfyGYy0MA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/goktugin/skills-introduction-to-github/blob/main/Filtering_SNPs_csv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKz8okssHLyi",
        "outputId": "92ebfb23-cce5-473e-fc5c-7e144db44589"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cyvcf2\n",
            "  Downloading cyvcf2-0.31.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from cyvcf2) (2.0.2)\n",
            "Collecting coloredlogs (from cyvcf2)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from cyvcf2) (8.2.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->cyvcf2)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Downloading cyvcf2-0.31.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: humanfriendly, coloredlogs, cyvcf2\n",
            "Successfully installed coloredlogs-15.0.1 cyvcf2-0.31.1 humanfriendly-10.0\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,683 kB]\n",
            "Get:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Get:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease [24.6 kB]\n",
            "Get:12 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,966 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [4,387 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,552 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,245 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,944 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,255 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,540 kB]\n",
            "Get:19 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,730 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [83.2 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.2 kB]\n",
            "Get:22 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [46.6 kB]\n",
            "Get:23 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy/main amd64 Packages [77.3 kB]\n",
            "Fetched 32.0 MB in 5s (7,083 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libhts3 libhtscodecs2\n",
            "Suggested packages:\n",
            "  python3-numpy python3-matplotlib texlive-latex-recommended\n",
            "The following NEW packages will be installed:\n",
            "  bcftools libhts3 libhtscodecs2\n",
            "0 upgraded, 3 newly installed, 0 to remove and 91 not upgraded.\n",
            "Need to get 1,140 kB of archives.\n",
            "After this operation, 3,448 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libhtscodecs2 amd64 1.1.1-3 [53.2 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libhts3 amd64 1.13+ds-2build1 [390 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 bcftools amd64 1.13-1 [697 kB]\n",
            "Fetched 1,140 kB in 1s (1,540 kB/s)\n",
            "Selecting previously unselected package libhtscodecs2:amd64.\n",
            "(Reading database ... 126102 files and directories currently installed.)\n",
            "Preparing to unpack .../libhtscodecs2_1.1.1-3_amd64.deb ...\n",
            "Unpacking libhtscodecs2:amd64 (1.1.1-3) ...\n",
            "Selecting previously unselected package libhts3:amd64.\n",
            "Preparing to unpack .../libhts3_1.13+ds-2build1_amd64.deb ...\n",
            "Unpacking libhts3:amd64 (1.13+ds-2build1) ...\n",
            "Selecting previously unselected package bcftools.\n",
            "Preparing to unpack .../bcftools_1.13-1_amd64.deb ...\n",
            "Unpacking bcftools (1.13-1) ...\n",
            "Setting up libhtscodecs2:amd64 (1.1.1-3) ...\n",
            "Setting up libhts3:amd64 (1.13+ds-2build1) ...\n",
            "Setting up bcftools (1.13-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from scipy.stats import f_oneway\n",
        "import numpy as np\n",
        "\n",
        "!pip install cyvcf2\n",
        "from cyvcf2 import VCF\n",
        "import os\n",
        "import subprocess\n",
        "import re\n",
        "import time\n",
        "\n",
        "!apt-get update\n",
        "!apt-get install -y bcftools"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# .vcf.gz dosyasının tam URL'sini buraya yazın\n",
        "vcf_gz_url = \"ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/ALL.chr21.phase3_shapeit2_mvncall_integrated_v5b.20130502.genotypes.vcf.gz\"\n",
        "chromosome = re.search(r'\\bchr[0-9XYM]+', vcf_gz_url).group()\n",
        "\n",
        "# .vcf.gz.tbi dosyasının URL'si\n",
        "tbi_url = vcf_gz_url + \".tbi\"\n",
        "\n",
        "# Dosya adlarını URL'lerden çıkaralım (daha temiz bir kod için)\n",
        "vcf_gz_filename = vcf_gz_url.split(\"/\")[-1]\n",
        "tbi_filename = tbi_url.split(\"/\")[-1]\n",
        "\n",
        "print(f\".vcf.gz dosyası indiriliyor: {vcf_gz_filename}\")\n",
        "!wget {vcf_gz_url}\n",
        "\n",
        "print(f\"\\n.tbi dosyası indiriliyor: {tbi_filename}\")\n",
        "!wget {tbi_url}\n",
        "\n",
        "print(\"\\nİndirme işlemleri tamamlandı. Dosyalar kontrol ediliyor:\")\n",
        "!ls -lh {vcf_gz_filename} {tbi_filename}\n",
        "\n",
        "\n",
        "# İndirilecek VCF dosyası\n",
        "vcf_file_path = f\"ALL.{chromosome}.phase3_shapeit2_mvncall_integrated_v5b.20130502.genotypes.vcf.gz\"\n",
        "# Panel dosyasının URL'si ve Colab'e indirilecek adı\n",
        "panel_url = \"ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/integrated_call_samples_v3.20130502.ALL.panel\"\n",
        "panel_file_path = \"integrated_call_samples_v3.20130502.ALL.panel\" # wget ile indirilecek dosya adı\n",
        "\n",
        "print(f\"Panel dosyası indiriliyor: {panel_file_path}\")\n",
        "!wget -O {panel_file_path} {panel_url} # -O ile dosya adını belirtiyoruz\n",
        "print(\"\\nİndirme tamamlandı.\")\n",
        "!ls -lh {panel_file_path} # Dosyanın indiğini ve boyutunu kontrol et\n",
        "\n",
        "\n",
        "try:\n",
        "    # Panel dosyası genellikle tab ile ayrılmıştır ve ilk satır başlıktır. Sütun adları: 'sample', 'pop', 'super_pop', 'gender'\n",
        "    panel_df = pd.read_csv(panel_file_path, sep='\\t')\n",
        "\n",
        "    # VCF dosyasındaki örnek ID'lerini alalım (cyvcf2 kullanarak)\n",
        "    vcf_reader_for_samples = VCF(vcf_file_path) #Dosyayı okumaya hazır bir nesne (vcf_reader_for_samples) oluşturulmuş olur.\n",
        "    vcf_samples = vcf_reader_for_samples.samples #.samples özelliği, VCF dosyasındaki bireylerin/örneklerin isimlerinin bir listesini döndürür.\n",
        "    vcf_reader_for_samples.close() # Okuyucuyu kapatalım\n",
        "\n",
        "    # Paneldeki örnekleri VCF'dekilerle filtreme\n",
        "    panel_df_filtered = panel_df[panel_df['sample'].isin(vcf_samples)]\n",
        "\n",
        "    # 'sample' ID'lerini 'super_pop' kodlarına eşleyen bir sözlük oluşturalım. Yani {'Sample1': 'EUR', 'Sample2': 'AFR', ...} gibi bir sonuç\n",
        "    sample_to_pop = pd.Series(panel_df_filtered.super_pop.values, index=panel_df_filtered['sample']).to_dict()\n",
        "\n",
        "    if not sample_to_pop:\n",
        "        print(\"UYARI: 'sample_to_pop' sözlüğü boş. Panel dosyası içeriği veya VCF örnekleriyle eşleşme kontrol edilmeli.\")\n",
        "        print(f\"Panel dosyasında bulunan örnek sayısı: {len(panel_df)}\")\n",
        "        print(f\"VCF dosyasında bulunan örnek sayısı: {len(vcf_samples)}\")\n",
        "        print(f\"Panelde VCF ile eşleşen örnek sayısı: {len(panel_df_filtered)}\")\n",
        "\n",
        "    print(f\"Panel dosyasından {len(sample_to_pop)} örnek için popülasyon bilgisi başarıyla yüklendi.\")\n",
        "    print(\"Örnek bir eşleşme (ilk 5):\", dict(list(sample_to_pop.items())[:5]))\n",
        "    print(\"Paneldeki benzersiz süper popülasyonlar:\", panel_df_filtered['super_pop'].unique())\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"HATA: Panel dosyası bulunamadı: {panel_file_path}\")\n",
        "    print(\"Lütfen panel dosyasının doğru indirildiğinden ve 'panel_file_path' değişkeninin doğru olduğundan emin olun.\")\n",
        "    sample_to_pop = {}\n",
        "except KeyError as e:\n",
        "    print(f\"HATA: Panel dosyasında beklenen sütun bulunamadı: {e}\")\n",
        "    print(\"Panel dosyasının sütun adlarını ('sample', 'super_pop' vb.) ve formatını kontrol edin.\")\n",
        "    sample_to_pop = {}\n",
        "except Exception as e:\n",
        "    print(f\"Panel dosyası işlenirken bir hata oluştu: {e}\")\n",
        "    sample_to_pop = {}\n",
        "\n",
        "\n",
        "\n",
        "    # MAF (Minör Alel Frekansı) Filtrelemesi --> yaygın olan varyantları filtreliyoruz\n",
        "\n",
        "filtered_vcf_path = f\"ALL.{chromosome}.phase3_filtered_maf005.vcf.gz\" # Yeni filtrelenmiş dosya adı\n",
        "\n",
        "# ÖNEMLİ: bcftools'un MAF hesaplaması için INFO tag'ında AF (Allele Frequency) olması gerekir.\n",
        "# 1000 Genomes VCF'lerinde bu genellikle vardı veya INFO/AC (Allele Count) ve INFO/AN (Allele Number) üzerinden hesaplanabilir.\n",
        "# Eğer doğrudan MAF yoksa, 'AF > 0.05 && AF < 0.95' gibi bir filtre de kullanılabilir.\n",
        "# En basit haliyle AF (Alternatif Alel Frekansı) üzerinden gidelim:\n",
        "print(f\"MAF filtrelemesi başlıyor (AF > 0.05 ve AF < 0.95)...\")\n",
        "!bcftools view -i 'INFO/AF > 0.05 && INFO/AF < 0.95' {vcf_file_path} -Oz -o {filtered_vcf_path} # i --> include, INFO/AF --> INFO sütununda bulunan \"AF\"\n",
        "# minör alel frekansı (MAF) en az 0.05 olan varyantları tutar.\n",
        "# Yani hem çok nadir varyantları (MAF &lt; 0.05) hem de neredeyse popülasyonda sabitlenmiş\n",
        "# (alternatif alelin frekansı > 0.95, dolayısıyla referans alelin frekansı &lt; 0.05, yani MAF &lt; 0.05) varyantları çıkarır.\n",
        "# Kısacası, popülasyonda belirli bir düzeyde polimorfik (çeşitlilik gösteren) olan varyantları seçer.\n",
        "print(f\"Filtrelenmiş VCF dosyası oluşturuldu: {filtered_vcf_path}\")\n",
        "\n",
        "# Filtrelenmiş VCF için indeks oluşturmayı unutmayın (verimlilik artışı için)\n",
        "print(f\"Filtrelenmiş VCF için indeks oluşturuluyor...\")\n",
        "!bcftools index {filtered_vcf_path}\n",
        "print(f\"İndeks oluşturuldu: {filtered_vcf_path}.tbi\")\n",
        "\n",
        "\n",
        "# ----- GİRİŞ VE ÇIKIŞ DOSYA ADLARI -----\n",
        "# MAF ile filtrelediğiniz VCF dosyasının adı (bir önceki adımdaki çıktı)\n",
        "maf_filtered_vcf = f\"ALL.{chromosome}.phase3_filtered_maf005.vcf.gz\"\n",
        "\n",
        "# PLINK işlemleri için kullanılacak geçici dosya ön ekleri\n",
        "plink_input_prefix = f\"{chromosome}_maf_filtered_for_plink\"\n",
        "plink_ld_pruning_prefix = f\"{chromosome}_ld_pruned_list\"\n",
        "plink_final_pruned_prefix = f\"{chromosome}_maf_ld_pruned\"\n",
        "\n",
        "# LD Pruning sonrası ANOVA için kullanılacak nihai VCF dosyasının adı\n",
        "ld_pruned_vcf_for_anova = f\"ALL.{chromosome}.filtered_maf005_ldpruned.vcf.gz\"\n",
        "# ----- ----- ----- ----- ----- ----- -----\n",
        "\n",
        "# 1. GÜNCELLENMİŞ PLINK Kurulumu\n",
        "print(\"PLINK indiriliyor ve ayarlanıyor...\")\n",
        "# Önceki olası kalıntıları temizleyelim (daha güvenli bir kurulum için)\n",
        "!rm -f plink plink.zip plink.* 큰*.* *.log # plink, plink.zip ve PLINK ile gelen diğer dosyaları sil\n",
        "\n",
        "# GÜNCELLENMİŞ İNDİRME LİNKİ:\n",
        "plink_download_url = \"https://s3.amazonaws.com/plink1-assets/plink_linux_x86_64_20231211.zip\"\n",
        "!wget {plink_download_url} -O plink.zip\n",
        "\n",
        "# wget'in başarılı olup olmadığını kontrol edelim (dosya boyutu > 0 olmalı)\n",
        "if os.path.exists(\"plink.zip\") and os.path.getsize(\"plink.zip\") > 0:\n",
        "    print(\"PLINK zip dosyası başarıyla indirildi.\")\n",
        "    !unzip -o plink.zip # -o ile üzerine yazma komutu (overwrite)\n",
        "    print(\"\\nZip'ten çıkarılan dosyalar:\")\n",
        "    !ls -l # Zip'ten çıkan dosyaları listele, 'plink' adında bir dosya görmeliyiz\n",
        "\n",
        "    if os.path.isfile(\"plink\"):\n",
        "        !chmod +x plink\n",
        "        print(\"PLINK başarıyla ayıklandı ve çalıştırılabilir yapıldı.\")\n",
        "    else:\n",
        "        print(\"HATA: PLINK çalıştırılabilir dosyası ('plink') zip'ten çıkarıldıktan sonra bulunamadı!\")\n",
        "        print(\"Lütfen yukarıdaki 'ls -l' çıktısını kontrol edin. Eğer farklı bir isimle çıkarıldıysa, kodda './plink' yerine o ismi kullanmanız gerekir.\")\n",
        "        raise FileNotFoundError(\"PLINK executable ('plink') not found after unzip.\")\n",
        "else:\n",
        "    print(f\"HATA: PLINK zip dosyası ({plink_download_url}) indirilemedi veya boş. Lütfen linki kontrol edin veya çalışan başka bir PLINK 1.9 indirme linki bulun.\")\n",
        "    raise FileNotFoundError(\"PLINK zip file download failed or the file is empty.\")\n",
        "\n",
        "\n",
        "    # ----- GİRİŞ VE ÇIKIŞ DOSYA ADLARI (bir öncekiyle aynı) -----\n",
        "maf_filtered_vcf = f\"ALL.{chromosome}.phase3_filtered_maf005.vcf.gz\"\n",
        "vcf_with_ids = f\"ALL.{chromosome}.phase3_filtered_maf005.ids.vcf.gz\"\n",
        "plink_input_prefix = f\"{chromosome}_maf_ids_for_plink\"\n",
        "plink_ld_pruning_prefix = f\"{chromosome}_ld_pruned_list\"\n",
        "plink_final_pruned_prefix = f\"{chromosome}_maf_ids_ld_pruned\"\n",
        "ld_pruned_vcf_for_anova = f\"ALL.{chromosome}.filtered_maf005_ids_ldpruned.vcf.gz\"\n",
        "plink_executable = \"./plink\"\n",
        "# ----- ----- ----- ----- ----- ----- ----- ----- -----\n",
        "\n",
        "def run_command(cmd_list, log_prefix_for_error=\"cmd\"):\n",
        "    \"\"\"Yardımcı fonksiyon: Komut çalıştırır, başarı/hata durumunu yönetir.\"\"\"\n",
        "    print(f\"Komut çalıştırılıyor: {' '.join(cmd_list[:4])} ...\") # Komutun başını göster\n",
        "    log_file = f\"{log_prefix_for_error}.log\"\n",
        "    # subprocess.run'dan önce log dosyasını silmeyin, PLINK kendisi oluşturur/üzerine yazar\n",
        "\n",
        "    result = subprocess.run(cmd_list, capture_output=True, text=True, check=False)\n",
        "\n",
        "    if result.returncode == 0:\n",
        "        print(f\"BAŞARILI: Komut tamamlandı. (Log: {log_file if os.path.exists(log_file) else 'oluşturulmadı'})\")\n",
        "        # Başarılıysa sadece kısa bir STDERR özeti (uyarılar için)\n",
        "        if result.stderr.strip():\n",
        "            print(\"  PLINK STDERR (Uyarılar olabilir):\")\n",
        "            # Sadece ilk birkaç satırı veya önemli uyarıları göster\n",
        "            stderr_lines = result.stderr.strip().split('\\n')\n",
        "            for line in stderr_lines[:5]: # İlk 5 satır\n",
        "                print(f\"    {line}\")\n",
        "            if len(stderr_lines) > 5:\n",
        "                print(\"    ...\")\n",
        "        return result.stdout # Başarılı durumda STDOUT'u döndür (işlemek için)\n",
        "    else:\n",
        "        print(f\"HATA: Komut başarısız oldu. Dönüş kodu: {result.returncode}\")\n",
        "        print(\"  --- PLINK STDOUT ---\")\n",
        "        print(result.stdout)\n",
        "        print(\"  --- PLINK STDERR ---\")\n",
        "        print(result.stderr)\n",
        "        if os.path.exists(log_file):\n",
        "            print(f\"\\n  --- {log_file} içeriği ---\")\n",
        "            with open(log_file, 'r') as f:\n",
        "                print(f.read())\n",
        "            print(f\"  --- {log_file} sonu ---\")\n",
        "        raise RuntimeError(f\"Komut başarısız: {' '.join(cmd_list[:3])}...\")\n",
        "\n",
        "# --- Adım 0: Dosya Kontrolleri (kısaltılmış) ---\n",
        "if not (os.path.isfile(plink_executable) and os.access(plink_executable, os.X_OK)):\n",
        "    raise FileNotFoundError(f\"PLINK executable not found or not executable: {plink_executable}\")\n",
        "if not os.path.exists(maf_filtered_vcf):\n",
        "    raise FileNotFoundError(f\"Input VCF file not found: {maf_filtered_vcf}\")\n",
        "print(\"PLINK ve girdi VCF dosyası bulundu.\")\n",
        "\n",
        "# --- Adım 1: bcftools annotate\n",
        "print(f\"\\nAdım 1: '{maf_filtered_vcf}' dosyasındaki eksik ID'ler bcftools annotate ile dolduruluyor...\")\n",
        "bcftools_annotate_cmd = [\"bcftools\", \"annotate\", \"--set-id\", \"+'%CHROM:%POS:%REF:%FIRST_ALT'\", maf_filtered_vcf, \"-Oz\", \"-o\", vcf_with_ids]\n",
        "run_command(bcftools_annotate_cmd, \"bcftools_annotate\")\n",
        "!bcftools index {vcf_with_ids}\n",
        "print(f\"{vcf_with_ids} dosyası oluşturuldu ve indekslendi.\")\n",
        "\n",
        "# --- Adım 2: VCF to BED (PLINK) ---\n",
        "print(f\"\\nAdım 2: '{vcf_with_ids}' dosyası PLINK BED formatına dönüştürülüyor...\")\n",
        "plink_cmd_step2 = [plink_executable, \"--vcf\", vcf_with_ids, \"--allow-extra-chr\", \"--make-bed\", \"--out\", plink_input_prefix]\n",
        "stdout_step2 = run_command(plink_cmd_step2, plink_input_prefix)\n",
        "# PLINK'in yüklediği varyant sayısını STDOUT'tan veya log'dan alabiliriz (isteğe bağlı)\n",
        "variants_loaded_match = re.search(r\"(\\d+) variants loaded from .bim file\", stdout_step2)\n",
        "if variants_loaded_match:\n",
        "    print(f\"  {variants_loaded_match.group(1)} varyant PLINK BED formatına yüklendi.\")\n",
        "print(f\"PLINK BED formatına dönüştürme başarılı.\")\n",
        "\n",
        "# LD Pruning, yüksek LD'de olan SNP gruplarından sadece bir veya birkaç temsilci SNP'yi tutarak diğerlerini veri setinden çıkarır. Bu sayede, PCA için daha \"bağımsız\" ve daha az sayıda SNP içeren bir set elde edilir, bu da daha güvenilir ve yorumlanabilir popülasyon yapısı sonuçları verir.\n",
        "# --- Adım 3: LD Pruning (PLINK) ---\n",
        "window_size_snps = 50; step_size_snps = 5; r2_threshold = 0.2\n",
        "print(f\"\\nAdım 3: LD Pruning işlemi başlıyor...\")\n",
        "plink_cmd_step3 = [plink_executable, \"--bfile\", plink_input_prefix, \"--allow-extra-chr\", \"--indep-pairwise\", str(window_size_snps), str(step_size_snps), str(r2_threshold), \"--out\", plink_ld_pruning_prefix]\n",
        "stdout_step3 = run_command(plink_cmd_step3, plink_ld_pruning_prefix)\n",
        "pruned_match = re.search(r\"Pruning complete.  (\\d+) of \\d+ variants removed\", stdout_step3)\n",
        "if pruned_match:\n",
        "    print(f\"  LD Pruning tamamlandı. {pruned_match.group(1)} varyant çıkarıldı.\")\n",
        "else: # Logdan okumayı deneyebiliriz veya sadece genel mesaj\n",
        "    print(f\"  LD Pruning tamamlandı. Ayrıntılar için {plink_ld_pruning_prefix}.log dosyasına bakınız.\")\n",
        "print(f\"LD Pruning başarılı.\")\n",
        "\n",
        "# --- Adım 4: Budanmış Seti Oluşturma (PLINK) ---\n",
        "print(f\"\\nAdım 4: Budanmış SNP listesi kullanılarak yeni PLINK dosyası oluşturuluyor...\")\n",
        "plink_cmd_step4 = [plink_executable, \"--bfile\", plink_input_prefix, \"--allow-extra-chr\", \"--extract\", f\"{plink_ld_pruning_prefix}.prune.in\", \"--make-bed\", \"--out\", plink_final_pruned_prefix]\n",
        "stdout_step4 = run_command(plink_cmd_step4, plink_final_pruned_prefix)\n",
        "variants_remaining_match = re.search(r\"--extract: (\\d+) variants remaining\", stdout_step4)\n",
        "if variants_remaining_match:\n",
        "    print(f\"  {variants_remaining_match.group(1)} SNP ile yeni PLINK seti oluşturuldu.\")\n",
        "print(f\"Yeni budanmış PLINK dosyası başarıyla oluşturuldu.\")\n",
        "\n",
        "# --- Adım 5: VCF'ye Geri Dönüşüm (PLINK) ---\n",
        "output_vcf_prefix_for_plink = ld_pruned_vcf_for_anova.replace(\".vcf.gz\", \"\")\n",
        "print(f\"\\nAdım 5: Son budanmış PLINK seti VCF formatına dönüştürülüyor: {ld_pruned_vcf_for_anova}\")\n",
        "plink_cmd_step5 = [plink_executable, \"--bfile\", plink_final_pruned_prefix, \"--allow-extra-chr\", \"--recode\", \"vcf-iid\", \"bgz\", \"--out\", output_vcf_prefix_for_plink]\n",
        "run_command(plink_cmd_step5, output_vcf_prefix_for_plink)\n",
        "if os.path.exists(f\"{output_vcf_prefix_for_plink}.vcf.gz\"):\n",
        "    if f\"{output_vcf_prefix_for_plink}.vcf.gz\" != ld_pruned_vcf_for_anova:\n",
        "         os.rename(f\"{output_vcf_prefix_for_plink}.vcf.gz\", ld_pruned_vcf_for_anova)\n",
        "print(f\"VCF formatına dönüştürme başarılı.\")\n",
        "\n",
        "# --- Adım 6: İndeksleme (bcftools) ---\n",
        "print(f\"\\nAdım 6: {ld_pruned_vcf_for_anova} dosyası bcftools ile indeksleniyor...\")\n",
        "!bcftools index {ld_pruned_vcf_for_anova}\n",
        "print(f\"İndeksleme başarılı.\")\n",
        "\n",
        "print(f\"\\nLD Pruning tamamlandı! ANOVA analizi için kullanılacak dosya: {ld_pruned_vcf_for_anova}\")\n",
        "!ls -lh {ld_pruned_vcf_for_anova}* # Son dosyayı ve indeksini göster\n",
        "\n",
        "\n",
        "# ----- ANOVA İÇİN GİRDİ DOSYASI -----\n",
        "# LD Pruning sonrası oluşan VCF dosyasının adı:\n",
        "vcf_file_path = f\"ALL.{chromosome}.filtered_maf005_ids_ldpruned.vcf.gz\"\n",
        "# ----- ----- ----- ----- ----- -----\n",
        "\n",
        "# sample_to_pop sözlüğünün ve diğer gerekli değişkenlerin\n",
        "# (min_samples_per_pop_for_calc, top_n_snps_to_select)\n",
        "# bir önceki hücrelerden yüklendiğini varsayıyoruz.\n",
        "# Eğer tanımlı değillerse, burada tekrar tanımlamanız gerekir:\n",
        "\n",
        "if 'sample_to_pop' not in locals() or not sample_to_pop:\n",
        "    print(\"HATA: 'sample_to_pop' sözlüğü bulunamadı. Lütfen popülasyon bilgilerini yükleme adımını tekrar çalıştırın.\")\n",
        "    raise NameError(\"'sample_to_pop' sözlüğü tanımlı değil veya boş.\")\n",
        "\n",
        "if 'min_samples_per_pop_for_calc' not in locals():\n",
        "    min_samples_per_pop_for_calc = 5 # Popülasyon başına ANOVA için min örnek\n",
        "\n",
        "\n",
        "# --- ANOVA Analiz Kodu ---\n",
        "try:\n",
        "    vcf_reader = VCF(vcf_file_path)\n",
        "except OSError as e:\n",
        "    print(f\"HATA: VCF dosyası ({vcf_file_path}) veya indeksi bulunamadı/okunamadı: {e}\")\n",
        "    raise\n",
        "\n",
        "samples_in_vcf = vcf_reader.samples\n",
        "pop_to_samples = defaultdict(list) #Amacımız, her bir süper popülasyon kodunu (örn: 'EUR', 'AFR') anahtar olarak ve o popülasyona ait örnek ID'lerinin bir listesini de değer olarak tutan bir sözlük oluşturmaktır. pop_to_samples bu sözlük olacaktır.\n",
        "for sample_id, super_pop_code in sample_to_pop.items():\n",
        "    if sample_id in samples_in_vcf:\n",
        "         pop_to_samples[super_pop_code].append(sample_id)\n",
        "\n",
        "sample_indices = {sample_name: i for i, sample_name in enumerate(samples_in_vcf)}\n",
        "all_calculated_snps_with_pvalues = [] #ANOVA analizi sırasında, her bir SNP için bir p-değeri hesaplayacağız. Eğer bu p-değeri geçerliyse (NaN değilse), o SNP'ye ait bilgileri (kromozom, pozisyon, ID, p-değeri ve ANOVA'ya dahil edilen popülasyonlar) bir demet (tuple) olarak bu listeye ekleyeceğiz. Döngü tamamlandığında, bu liste, p-değeri hesaplanabilmiş tüm SNP'leri içerecektir. Daha sonra bu listeyi p-değerine göre sıralayıp en iyi N tanesini seçeceğiz.\n",
        "processed_variants_count = 0 #VCF dosyasındaki varyantlar (SNP'ler) üzerinde dönerken, o ana kadar kaç tane varyantın işlendiğini (yani döngünün kaçıncı adımında olduğumuzu) saymak için kullanılacak bir sayaç\n",
        "total_variants_in_file = 0 # VCF dosyasındaki toplam varyant satırı sayısını (yorum satırları ve başlık hariç) saymak için kullanılır. Döngü her bir varyant için çalıştığında bu sayaç artırılır. Analiz bittikten sonra, VCF dosyasında toplam kaç varyant olduğunu ve bunlardan kaçının işlendiğini raporlamak için kullanılır.\n",
        "\n",
        "if pop_to_samples:\n",
        "    print(f\"ANOVA analizi '{vcf_file_path}' dosyası üzerinde başlıyor...\")\n",
        "    print(f\"Popülasyon başına ANOVA için minimum örnek sayısı: {min_samples_per_pop_for_calc}\")\n",
        "    print(f\"Analiz sonunda bütün SNP'ler seçilecektir.\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    for variant in vcf_reader: # Bu döngü şimdi çok daha hızlıdır\n",
        "        total_variants_in_file += 1\n",
        "        processed_variants_count += 1\n",
        "\n",
        "        if processed_variants_count % 2000 == 0: # İlerleme bildirimi\n",
        "            print(f\"{processed_variants_count} varyant işlendi...\")\n",
        "\n",
        "        if not (len(variant.ALT) == 1 and len(variant.REF) == 1 and len(variant.ALT[0]) == 1):\n",
        "            continue\n",
        "\n",
        "        # Her yeni varyant için, ANOVA'ya girecek genotip değerlerini ve\n",
        "        # bu ANOVA'ya dahil edilecek popülasyonları saklamak üzere boş listeler oluşturulur.\n",
        "        pop_genotype_values_for_anova = []\n",
        "        populations_in_anova = []\n",
        "\n",
        "        # 'pop_to_samples' sözlüğü üzerinde döngüye girilir. Bu sözlük, her bir süper popülasyon kodunu ('pop_code')\n",
        "        # o popülasyona ait örnek ID'lerinin bir listesine ('pop_sample_list') eşler.\n",
        "        # Örn: pop_code = 'EUR', pop_sample_list = ['HG00096', 'HG00097', ...]\n",
        "        for pop_code, pop_sample_list in pop_to_samples.items():\n",
        "            # O anki popülasyona ('pop_code') ait olup, aynı zamanda VCF dosyamızda da bulunan\n",
        "            # (yani 'sample_indices' sözlüğünde anahtarı olan) örnek ID'lerini seçer.\n",
        "            current_pop_samples_in_vcf = [s_id for s_id in pop_sample_list if s_id in sample_indices]\n",
        "\n",
        "            # Eğer bu popülasyondaki (VCF'de de bulunan) örnek sayısı, belirlediğimiz minimum eşikten\n",
        "            # ('min_samples_per_pop_for_calc', örn: 5) az ise, bu popülasyon bu SNP için ANOVA'ya\n",
        "            # dahil edilmez ve döngünün bir sonraki popülasyonuna geçilir ('continue').\n",
        "            # Bu, çok az bireye sahip grupların istatistiksel sonuçları aşırı etkilemesini önler.\n",
        "            if len(current_pop_samples_in_vcf) < min_samples_per_pop_for_calc:\n",
        "                continue\n",
        "\n",
        "            # Bu popülasyondaki ('current_pop_samples_in_vcf') bireylerin, o anki varyant için\n",
        "            # sahip oldukları alternatif alel sayılarını toplayacağımız boş bir liste.\n",
        "            individual_alt_allele_counts_in_pop = []\n",
        "\n",
        "            # Popülasyondaki her bir örnek ('sample_name') için döngüye girilir.\n",
        "            for sample_name in current_pop_samples_in_vcf:\n",
        "                idx = sample_indices[sample_name]  # Örnek ID'sinin VCF'deki sayısal indeksini alır.\n",
        "                genotype = variant.genotypes[idx]  # O örneğin, o anki varyanttaki genotipini alır.\n",
        "                                                   # genotype bir listedir: [alel1_indeksi, alel2_indeksi, faz_bilgisi]\n",
        "                                                   # alel_indeksi: 0 (referans alel), 1 (ilk alternatif alel), -1 (eksik veri).\n",
        "\n",
        "                alt_count_for_sample = 0     # Bu birey için o anki SNP'deki alternatif alel sayısı.\n",
        "                valid_alleles_for_sample = 0 # Bu birey için geçerli (eksik olmayan) alel sayısı.\n",
        "\n",
        "                # Bireyin ilk alelini kontrol et:\n",
        "                if genotype[0] != -1:  # Eğer alel eksik veri (-1) değilse:\n",
        "                    alt_count_for_sample += genotype[0] # alel1_indeksi (0 veya 1) eklenir.\n",
        "                    valid_alleles_for_sample += 1       # Geçerli alel sayısını artır.\n",
        "\n",
        "                # Bireyin ikinci alelini kontrol et:\n",
        "                if genotype[1] != -1:  # Eğer alel eksik veri (-1) değilse:\n",
        "                    alt_count_for_sample += genotype[1] # alel2_indeksi (0 veya 1) eklenir.\n",
        "                    valid_alleles_for_sample += 1       # Geçerli alel sayısını artır.\n",
        "\n",
        "                # Eğer bireyin en az bir aleli geçerliyse (yani genotip tamamen ' ./. ' değilse),\n",
        "                # hesaplanan alternatif alel sayısını (0, 1 veya 2 olabilir) listeye ekle.\n",
        "                if valid_alleles_for_sample > 0 :\n",
        "                    individual_alt_allele_counts_in_pop.append(alt_count_for_sample)\n",
        "\n",
        "            # Eğer bu popülasyondan, minimum örnek sayısını ('min_samples_per_pop_for_calc')\n",
        "            # karşılayacak kadar bireyden genotip verisi (alternatif alel sayısı) toplayabildiysek:\n",
        "            if len(individual_alt_allele_counts_in_pop) >= min_samples_per_pop_for_calc:\n",
        "                # Bu popülasyonun genotip değerlerini (bir NumPy dizisi olarak) ANOVA'ya girecek\n",
        "                # genel listeye ('pop_genotype_values_for_anova') ekle.\n",
        "                pop_genotype_values_for_anova.append(np.array(individual_alt_allele_counts_in_pop))\n",
        "                # Bu popülasyonun kodunu ('pop_code') da ANOVA'ya dahil edilen popülasyonlar\n",
        "                # listesine ('populations_in_anova') ekle.\n",
        "                populations_in_anova.append(pop_code)\n",
        "\n",
        "        # Tüm popülasyonlar için veri toplama işlemi bittikten sonra,\n",
        "        # ANOVA testi yapabilmek için en az 2 popülasyon grubundan veri toplamış olmamız gerekir.\n",
        "        if len(pop_genotype_values_for_anova) >= 2:\n",
        "            try:\n",
        "                # scipy.stats.f_oneway fonksiyonu ile tek yönlü ANOVA testi yapılır.\n",
        "                # '*' operatörü, 'pop_genotype_values_for_anova' listesindeki her bir NumPy dizisini\n",
        "                # (yani her bir popülasyonun alternatif alel sayıları listesini) fonksiyona\n",
        "                # ayrı bir argüman olarak gönderir. Her bir argüman, ANOVA'da bir grup olarak kabul edilir.\n",
        "                # Fonksiyon, F istatistiğini ve p-değerini döndürür.\n",
        "                f_stat, p_value = f_oneway(*pop_genotype_values_for_anova)\n",
        "\n",
        "                # Eğer hesaplanan p-değeri geçerli bir sayı ise (NaN - Not a Number - değilse):\n",
        "                if not np.isnan(p_value):\n",
        "                    # O anki varyantın Kromozom, Pozisyon, ID'si (bcftools ile atanan),\n",
        "                    # hesaplanan p-değeri ve bu ANOVA'ya dahil edilen popülasyonların listesini\n",
        "                    # bir demet (tuple) olarak 'all_calculated_snps_with_pvalues' listesine ekle.\n",
        "                    all_calculated_snps_with_pvalues.append((variant.CHROM, variant.POS, variant.ID, p_value, populations_in_anova))\n",
        "            except Exception as e:\n",
        "                # ANOVA hesaplaması sırasında bir hata oluşursa (örn: tüm gruplardaki tüm değerler aynıysa\n",
        "                # ve varyans sıfırsa, f_oneway uyarı verebilir veya hata fırlatabilir),\n",
        "                # bu hatayı sessizce atla ('pass') ve bir sonraki varyanta geç.\n",
        "                # İstenirse, hata ayıklama için buraya print(e) eklenebilir.\n",
        "                pass\n",
        "\n",
        "    end_time = time.time()\n",
        "    total_elapsed_time = end_time - start_time\n",
        "    print(f\"\\nANOVA hesaplamaları tamamlandı.\")\n",
        "    print(f\"Toplam {processed_variants_count} varyant (dosyadaki toplam: {total_variants_in_file}) işlendi.\")\n",
        "    print(f\"Toplam geçen süre: {total_elapsed_time:.2f} saniye.\")\n",
        "    print(f\"{len(all_calculated_snps_with_pvalues)} SNP için geçerli p-değeri hesaplandı.\")\n",
        "\n",
        "    all_calculated_snps_with_pvalues.sort(key=lambda x: x[3]) # p-value (indeks 3) göre sırala\n",
        "\n",
        "else:\n",
        "    print(\"Popülasyon bilgileri ('pop_to_samples') yüklenemediği için ANOVA analizi yapılamıyor.\")\n",
        "vcf_reader.close()\n",
        "\n",
        "\n",
        "if all_calculated_snps_with_pvalues:\n",
        "    print(\"Veriler CSV dosyasına kaydediliyor...\")\n",
        "\n",
        "    # DataFrame oluşturmak için veriyi hazırla\n",
        "    # 'populations_in_anova' listesini CSV'de daha okunabilir olması için bir stringe dönüştür\n",
        "    data_for_df = []\n",
        "    for item in all_calculated_snps_with_pvalues:\n",
        "        chrom, pos, snp_id, p_val, pops_list = item\n",
        "        # Popülasyon listesini virgülle ayrılmış bir stringe çevir\n",
        "        pops_str = \",\".join(pops_list)\n",
        "        data_for_df.append([chrom, pos, snp_id, p_val, pops_str])\n",
        "\n",
        "    # Pandas DataFrame oluştur\n",
        "    # Sütun adlarını tanımla\n",
        "    column_names = ['Chromosome', 'Position', 'SNP_ID', 'P_Value', 'Populations_in_ANOVA']\n",
        "    df_results = pd.DataFrame(data_for_df, columns=column_names)\n",
        "\n",
        "    # CSV dosyası olarak kaydet\n",
        "    csv_file_name = f\"anova_snps_{chromosome}_pvalues.csv\"\n",
        "    try:\n",
        "        # index=False, DataFrame indeksinin CSV dosyasına yazılmasını engeller\n",
        "        df_results.to_csv(csv_file_name, index=False)\n",
        "        print(f\"Sonuçlar başarıyla '{csv_file_name}' dosyasına kaydedildi.\")\n",
        "    except Exception as e:\n",
        "        print(f\"HATA: CSV dosyası kaydedilemedi: {e}\")\n",
        "else:\n",
        "    print(\"Kaydedilecek veri bulunamadı (all_calculated_snps_with_pvalues listesi boş).\")\n",
        "\n",
        "\n",
        "# Genotiplerin okunacağı VCF dosyası (LD pruning sonrası oluşan dosya)\n",
        "vcf_file_for_genotypes = f\"ALL.{chromosome}.filtered_maf005_ids_ldpruned.vcf.gz\"\n",
        "\n",
        "# Çıktı csv dosyasının adı\n",
        "output_csv_file = f\"genotip_matrisi_tum_snpler_{chromosome}.csv\" # Dosya adını güncelledim\n",
        "# ----- ----- ----- ----- ----- ----- ----- ----- ----- -----\n",
        "\n",
        "# print(f\"{len(all_calculated_snps_with_pvalues)} SNP için genotip matrisi oluşturulacak.\") # Bu satır artık geçerli değil\n",
        "print(f\"Girdi VCF dosyasındaki TÜM SNP'ler için genotip matrisi oluşturulacak: {vcf_file_for_genotypes}\")\n",
        "\n",
        "\n",
        "# 1. Artık önceden seçilmiş bir SNP seti kullanmıyoruz.\n",
        "# selected_snp_ids_set = {snp_info[2] for snp_info in all_calculated_snps_with_pvalues} # KALDIRILDI/YORUMLANDI\n",
        "# ordered_snp_ids_for_columns = [snp_info[2] for snp_info in all_calculated_snps_with_pvalues] # KALDIRILDI/YORUMLANDI\n",
        "all_snp_ids_in_vcf_order = [] # VCF'deki sırayla SNP ID'lerini tutmak için yeni liste\n",
        "\n",
        "# 2. VCF dosyasını aç ve örnek ID'lerini al\n",
        "try:\n",
        "    vcf_reader_gt = VCF(vcf_file_for_genotypes)\n",
        "except OSError as e:\n",
        "    print(f\"HATA: VCF dosyası ({vcf_file_for_genotypes}) veya indeksi bulunamadı/okunamadı: {e}\")\n",
        "    raise\n",
        "\n",
        "sample_ids = vcf_reader_gt.samples\n",
        "print(f\"{len(sample_ids)} örnek (birey) bulundu.\")\n",
        "\n",
        "# 3. Genotip verilerini toplamak için bir dictionary başlat\n",
        "# İlk sütun olarak örnek ID'lerini ekleyelim\n",
        "genotype_data_for_df = {'SampleID': sample_ids}\n",
        "\n",
        "# 4. VCF dosyasını oku ve TÜM SNP'ler için genotipleri çıkar\n",
        "processed_variants_count = 0 # İşlenen toplam varyant sayısı\n",
        "\n",
        "print(\"VCF dosyası okunuyor ve tüm SNP'ler için genotipler çıkarılıyor...\")\n",
        "for variant in vcf_reader_gt:\n",
        "    processed_variants_count += 1\n",
        "    variant_id = variant.ID # bcftools annotate ile oluşturduğumuz ID (örn: CHR:POS:REF:ALT)\n",
        "                           # Eğer ID yoksa veya farklı bir ID formatı kullanılıyorsa burayı ayarlamanız gerekebilir.\n",
        "                           # ID yoksa (None ise) veya boşsa, benzersiz bir ID oluşturabilirsiniz:\n",
        "    if variant_id is None or variant_id == \".\":\n",
        "        variant_id = f\"{variant.CHROM}:{variant.POS}:{variant.REF}:{variant.ALT[0]}\"\n",
        "\n",
        "\n",
        "    # ARTIK SNP ID'SİNİ KONTROL ETMİYORUZ, TÜMÜNÜ ALIYORUZ\n",
        "    # if variant_id in selected_snp_ids_set: # KALDIRILDI\n",
        "\n",
        "    # Alternatif alel sayılarını al (0, 1, 2). Eksikse np.nan\n",
        "    alt_allele_counts = []\n",
        "    for gt_array in variant.genotypes:\n",
        "        # gt_array = [allele_idx1, allele_idx2, is_phased]\n",
        "        # allele_idx: 0=REF, 1=ALT1, -1=eksik\n",
        "        allele1 = gt_array[0]\n",
        "        allele2 = gt_array[1]\n",
        "\n",
        "        if allele1 == -1 or allele2 == -1: # Eğer genotipin bir kısmı bile eksikse, tüm genotipi eksik sayalım\n",
        "            alt_allele_counts.append(np.nan)\n",
        "        else:\n",
        "            # Biallelik varsayımıyla (REF=0, ALT=1), toplamları alternatif alel sayısını verir\n",
        "            alt_allele_counts.append(allele1 + allele2)\n",
        "\n",
        "    genotype_data_for_df[variant_id] = alt_allele_counts\n",
        "    all_snp_ids_in_vcf_order.append(variant_id) # SNP ID'sini sırayla listeye ekle\n",
        "\n",
        "    if processed_variants_count % 1000 == 0: # Her 1000 varyantta bir ilerleme bildir\n",
        "        print(f\"  {processed_variants_count} varyant işlendi...\")\n",
        "\n",
        "vcf_reader_gt.close()\n",
        "print(f\"VCF okuma tamamlandı. {processed_variants_count} adet varyant (SNP) için genotip verisi toplandı.\")\n",
        "\n",
        "# Artık seçilmiş SNP'lerle karşılaştırma yapmıyoruz.\n",
        "# if found_selected_snps_in_vcf != len(selected_snp_ids_set):\n",
        "#     print(f\"UYARI: ANOVA ile seçilen {len(selected_snp_ids_set)} SNP'den sadece {found_selected_snps_in_vcf} tanesi VCF dosyasında bulundu!\")\n",
        "#     print(\"Bu durum, SNP ID eşleşmesinde bir sorun olduğuna veya VCF dosyasının beklenenden farklı olduğuna işaret edebilir.\")\n",
        "\n",
        "\n",
        "# 5. Pandas DataFrame oluştur\n",
        "# Sütunların doğru sırada ('SampleID' ve ardından VCF'deki sırayla SNP'ler) olması için\n",
        "final_columns_for_df = ['SampleID'] + all_snp_ids_in_vcf_order\n",
        "\n",
        "try:\n",
        "    genotype_df = pd.DataFrame(genotype_data_for_df)\n",
        "    # Sütunları 'SampleID' ve ardından VCF'deki SNP sırasına göre ayarla\n",
        "    genotype_df = genotype_df[final_columns_for_df]\n",
        "except KeyError as e:\n",
        "    print(f\"DataFrame oluşturulurken sütun hatası: {e}\")\n",
        "    print(\"Muhtemelen 'all_snp_ids_in_vcf_order' ile 'genotype_data_for_df' arasında bir uyumsuzluk var.\")\n",
        "    print(\"DataFrame mevcut sütunlarla oluşturuluyor...\")\n",
        "    genotype_df = pd.DataFrame(genotype_data_for_df) # Ham haliyle oluştur\n",
        "\n",
        "\n",
        "print(f\"\\nDataFrame oluşturuldu. Boyut: {genotype_df.shape[0]} satır, {genotype_df.shape[1]} sütun.\")\n",
        "print(\"İlk 5 satır ve ilk birkaç sütun:\")\n",
        "# Sütun sayısı 6'dan azsa hata vermemesi için min() kullandım\n",
        "print(genotype_df.iloc[:5, :min(6, genotype_df.shape[1])])\n",
        "\n",
        "# 6. DataFrame'i csv dosyasına kaydet\n",
        "print(f\"\\nDataFrame '{output_csv_file}' dosyasına kaydediliyor...\")\n",
        "try:\n",
        "    genotype_df.to_csv(output_csv_file, index=False)\n",
        "    print(f\"'{output_csv_file}' başarıyla kaydedildi!\")\n",
        "except Exception as e:\n",
        "    print(f\"CSV dosyasına kaydetme sırasında bir hata oluştu: {e}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCOUfr8wIJii",
        "outputId": "45df6bc7-7406-4a86-cb3c-1f2955857553"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".vcf.gz dosyası indiriliyor: ALL.chr21.phase3_shapeit2_mvncall_integrated_v5b.20130502.genotypes.vcf.gz\n",
            "--2025-05-22 17:20:06--  http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/ALL.chr21.phase3_shapeit2_mvncall_integrated_v5b.20130502.genotypes.vcf.gz\n",
            "Resolving ftp.1000genomes.ebi.ac.uk (ftp.1000genomes.ebi.ac.uk)... 193.62.193.167\n",
            "Connecting to ftp.1000genomes.ebi.ac.uk (ftp.1000genomes.ebi.ac.uk)|193.62.193.167|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 209774472 (200M) [application/x-gzip]\n",
            "Saving to: ‘ALL.chr21.phase3_shapeit2_mvncall_integrated_v5b.20130502.genotypes.vcf.gz’\n",
            "\n",
            "ALL.chr21.phase3_sh 100%[===================>] 200.06M  28.5MB/s    in 8.1s    \n",
            "\n",
            "2025-05-22 17:20:15 (24.8 MB/s) - ‘ALL.chr21.phase3_shapeit2_mvncall_integrated_v5b.20130502.genotypes.vcf.gz’ saved [209774472/209774472]\n",
            "\n",
            "\n",
            ".tbi dosyası indiriliyor: ALL.chr21.phase3_shapeit2_mvncall_integrated_v5b.20130502.genotypes.vcf.gz.tbi\n",
            "--2025-05-22 17:20:15--  http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/ALL.chr21.phase3_shapeit2_mvncall_integrated_v5b.20130502.genotypes.vcf.gz.tbi\n",
            "Resolving ftp.1000genomes.ebi.ac.uk (ftp.1000genomes.ebi.ac.uk)... 193.62.193.167\n",
            "Connecting to ftp.1000genomes.ebi.ac.uk (ftp.1000genomes.ebi.ac.uk)|193.62.193.167|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 35609 (35K) [application/x-gzip]\n",
            "Saving to: ‘ALL.chr21.phase3_shapeit2_mvncall_integrated_v5b.20130502.genotypes.vcf.gz.tbi’\n",
            "\n",
            "ALL.chr21.phase3_sh 100%[===================>]  34.77K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-05-22 17:20:15 (255 KB/s) - ‘ALL.chr21.phase3_shapeit2_mvncall_integrated_v5b.20130502.genotypes.vcf.gz.tbi’ saved [35609/35609]\n",
            "\n",
            "\n",
            "İndirme işlemleri tamamlandı. Dosyalar kontrol ediliyor:\n",
            "-rw-r--r-- 1 root root 201M Mar 16  2021 ALL.chr21.phase3_shapeit2_mvncall_integrated_v5b.20130502.genotypes.vcf.gz\n",
            "-rw-r--r-- 1 root root  35K Mar 16  2021 ALL.chr21.phase3_shapeit2_mvncall_integrated_v5b.20130502.genotypes.vcf.gz.tbi\n",
            "Panel dosyası indiriliyor: integrated_call_samples_v3.20130502.ALL.panel\n",
            "--2025-05-22 17:20:15--  ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/integrated_call_samples_v3.20130502.ALL.panel\n",
            "           => ‘integrated_call_samples_v3.20130502.ALL.panel’\n",
            "Resolving ftp.1000genomes.ebi.ac.uk (ftp.1000genomes.ebi.ac.uk)... 193.62.193.167\n",
            "Connecting to ftp.1000genomes.ebi.ac.uk (ftp.1000genomes.ebi.ac.uk)|193.62.193.167|:21... connected.\n",
            "Logging in as anonymous ... Logged in!\n",
            "==> SYST ... done.    ==> PWD ... done.\n",
            "==> TYPE I ... done.  ==> CWD (1) /vol1/ftp/release/20130502 ... done.\n",
            "==> SIZE integrated_call_samples_v3.20130502.ALL.panel ... 55156\n",
            "==> PASV ... done.    ==> RETR integrated_call_samples_v3.20130502.ALL.panel ... done.\n",
            "Length: 55156 (54K) (unauthoritative)\n",
            "\n",
            "integrated_call_sam 100%[===================>]  53.86K   193KB/s    in 0.3s    \n",
            "\n",
            "2025-05-22 17:20:17 (193 KB/s) - ‘integrated_call_samples_v3.20130502.ALL.panel’ saved [55156]\n",
            "\n",
            "\n",
            "İndirme tamamlandı.\n",
            "-rw-r--r-- 1 root root 54K May 22 17:20 integrated_call_samples_v3.20130502.ALL.panel\n",
            "Panel dosyasından 2504 örnek için popülasyon bilgisi başarıyla yüklendi.\n",
            "Örnek bir eşleşme (ilk 5): {'HG00096': 'EUR', 'HG00097': 'EUR', 'HG00099': 'EUR', 'HG00100': 'EUR', 'HG00101': 'EUR'}\n",
            "Paneldeki benzersiz süper popülasyonlar: ['EUR' 'EAS' 'AMR' 'SAS' 'AFR']\n",
            "MAF filtrelemesi başlıyor (AF > 0.05 ve AF < 0.95)...\n",
            "Filtrelenmiş VCF dosyası oluşturuldu: ALL.chr21.phase3_filtered_maf005.vcf.gz\n",
            "Filtrelenmiş VCF için indeks oluşturuluyor...\n",
            "İndeks oluşturuldu: ALL.chr21.phase3_filtered_maf005.vcf.gz.tbi\n",
            "PLINK indiriliyor ve ayarlanıyor...\n",
            "--2025-05-22 17:23:09--  https://s3.amazonaws.com/plink1-assets/plink_linux_x86_64_20231211.zip\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 54.231.194.224, 3.5.0.19, 16.182.33.112, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|54.231.194.224|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8953953 (8.5M) [application/zip]\n",
            "Saving to: ‘plink.zip’\n",
            "\n",
            "plink.zip           100%[===================>]   8.54M  13.3MB/s    in 0.6s    \n",
            "\n",
            "2025-05-22 17:23:11 (13.3 MB/s) - ‘plink.zip’ saved [8953953/8953953]\n",
            "\n",
            "PLINK zip dosyası başarıyla indirildi.\n",
            "Archive:  plink.zip\n",
            "  inflating: plink                   \n",
            "  inflating: LICENSE                 \n",
            "  inflating: toy.ped                 \n",
            "  inflating: toy.map                 \n",
            "  inflating: prettify                \n",
            "\n",
            "Zip'ten çıkarılan dosyalar:\n",
            "total 358472\n",
            "-rw-r--r-- 1 root root 106417541 May 22 17:23 ALL.chr21.phase3_filtered_maf005.vcf.gz\n",
            "-rw-r--r-- 1 root root     23621 May 22 17:23 ALL.chr21.phase3_filtered_maf005.vcf.gz.csi\n",
            "-rw-r--r-- 1 root root 209774472 Mar 16  2021 ALL.chr21.phase3_shapeit2_mvncall_integrated_v5b.20130502.genotypes.vcf.gz\n",
            "-rw-r--r-- 1 root root     35609 Mar 16  2021 ALL.chr21.phase3_shapeit2_mvncall_integrated_v5b.20130502.genotypes.vcf.gz.tbi\n",
            "-rw-r--r-- 1 root root     55156 May 22 17:20 integrated_call_samples_v3.20130502.ALL.panel\n",
            "-rw-rw-r-- 1 root root     35147 Dec 12  2023 LICENSE\n",
            "-rwxrwxr-x 1 root root  41730792 Dec 12  2023 plink\n",
            "-rw-r--r-- 1 root root   8953953 Dec 12  2023 plink.zip\n",
            "-rwxrwxr-x 1 root root     18336 Mar  6  2019 prettify\n",
            "drwxr-xr-x 1 root root      4096 May 14 13:38 sample_data\n",
            "-rw-rw-r-- 1 root root        27 Dec 12  2023 toy.map\n",
            "-rw-rw-r-- 1 root root        58 Dec 12  2023 toy.ped\n",
            "PLINK başarıyla ayıklandı ve çalıştırılabilir yapıldı.\n",
            "PLINK ve girdi VCF dosyası bulundu.\n",
            "\n",
            "Adım 1: 'ALL.chr21.phase3_filtered_maf005.vcf.gz' dosyasındaki eksik ID'ler bcftools annotate ile dolduruluyor...\n",
            "Komut çalıştırılıyor: bcftools annotate --set-id +'%CHROM:%POS:%REF:%FIRST_ALT' ...\n",
            "BAŞARILI: Komut tamamlandı. (Log: oluşturulmadı)\n",
            "ALL.chr21.phase3_filtered_maf005.ids.vcf.gz dosyası oluşturuldu ve indekslendi.\n",
            "\n",
            "Adım 2: 'ALL.chr21.phase3_filtered_maf005.ids.vcf.gz' dosyası PLINK BED formatına dönüştürülüyor...\n",
            "Komut çalıştırılıyor: ./plink --vcf ALL.chr21.phase3_filtered_maf005.ids.vcf.gz --allow-extra-chr ...\n",
            "BAŞARILI: Komut tamamlandı. (Log: chr21_maf_ids_for_plink.log)\n",
            "  115860 varyant PLINK BED formatına yüklendi.\n",
            "PLINK BED formatına dönüştürme başarılı.\n",
            "\n",
            "Adım 3: LD Pruning işlemi başlıyor...\n",
            "Komut çalıştırılıyor: ./plink --bfile chr21_maf_ids_for_plink --allow-extra-chr ...\n",
            "BAŞARILI: Komut tamamlandı. (Log: chr21_ld_pruned_list.log)\n",
            "  LD Pruning tamamlandı. 105804 varyant çıkarıldı.\n",
            "LD Pruning başarılı.\n",
            "\n",
            "Adım 4: Budanmış SNP listesi kullanılarak yeni PLINK dosyası oluşturuluyor...\n",
            "Komut çalıştırılıyor: ./plink --bfile chr21_maf_ids_for_plink --allow-extra-chr ...\n",
            "BAŞARILI: Komut tamamlandı. (Log: chr21_maf_ids_ld_pruned.log)\n",
            "  10056 SNP ile yeni PLINK seti oluşturuldu.\n",
            "Yeni budanmış PLINK dosyası başarıyla oluşturuldu.\n",
            "\n",
            "Adım 5: Son budanmış PLINK seti VCF formatına dönüştürülüyor: ALL.chr21.filtered_maf005_ids_ldpruned.vcf.gz\n",
            "Komut çalıştırılıyor: ./plink --bfile chr21_maf_ids_ld_pruned --allow-extra-chr ...\n",
            "BAŞARILI: Komut tamamlandı. (Log: ALL.chr21.filtered_maf005_ids_ldpruned.log)\n",
            "VCF formatına dönüştürme başarılı.\n",
            "\n",
            "Adım 6: ALL.chr21.filtered_maf005_ids_ldpruned.vcf.gz dosyası bcftools ile indeksleniyor...\n",
            "İndeksleme başarılı.\n",
            "\n",
            "LD Pruning tamamlandı! ANOVA analizi için kullanılacak dosya: ALL.chr21.filtered_maf005_ids_ldpruned.vcf.gz\n",
            "-rw-r--r-- 1 root root 6.3M May 22 17:24 ALL.chr21.filtered_maf005_ids_ldpruned.vcf.gz\n",
            "-rw-r--r-- 1 root root  19K May 22 17:24 ALL.chr21.filtered_maf005_ids_ldpruned.vcf.gz.csi\n",
            "ANOVA analizi 'ALL.chr21.filtered_maf005_ids_ldpruned.vcf.gz' dosyası üzerinde başlıyor...\n",
            "Popülasyon başına ANOVA için minimum örnek sayısı: 5\n",
            "Analiz sonunda bütün SNP'ler seçilecektir.\n",
            "2000 varyant işlendi...\n",
            "4000 varyant işlendi...\n",
            "6000 varyant işlendi...\n",
            "8000 varyant işlendi...\n",
            "10000 varyant işlendi...\n",
            "\n",
            "ANOVA hesaplamaları tamamlandı.\n",
            "Toplam 10056 varyant (dosyadaki toplam: 10056) işlendi.\n",
            "Toplam geçen süre: 38.33 saniye.\n",
            "7565 SNP için geçerli p-değeri hesaplandı.\n",
            "Veriler CSV dosyasına kaydediliyor...\n",
            "Sonuçlar başarıyla 'anova_snps_pvalues.csv' dosyasına kaydedildi.\n",
            "Girdi VCF dosyasındaki TÜM SNP'ler için genotip matrisi oluşturulacak: ALL.chr21.filtered_maf005_ids_ldpruned.vcf.gz\n",
            "2504 örnek (birey) bulundu.\n",
            "VCF dosyası okunuyor ve tüm SNP'ler için genotipler çıkarılıyor...\n",
            "  1000 varyant işlendi...\n",
            "  2000 varyant işlendi...\n",
            "  3000 varyant işlendi...\n",
            "  4000 varyant işlendi...\n",
            "  5000 varyant işlendi...\n",
            "  6000 varyant işlendi...\n",
            "  7000 varyant işlendi...\n",
            "  8000 varyant işlendi...\n",
            "  9000 varyant işlendi...\n",
            "  10000 varyant işlendi...\n",
            "VCF okuma tamamlandı. 10056 adet varyant (SNP) için genotip verisi toplandı.\n",
            "\n",
            "DataFrame oluşturuldu. Boyut: 2504 satır, 10057 sütun.\n",
            "İlk 5 satır ve ilk birkaç sütun:\n",
            "  SampleID  '21:9411410:C:T'  '21:9412441:TATA:T'  '21:9415026:TAATC:T'  \\\n",
            "0  HG00096                 1                    1                     0   \n",
            "1  HG00097                 2                    2                     1   \n",
            "2  HG00099                 1                    1                     1   \n",
            "3  HG00100                 1                    1                     1   \n",
            "4  HG00101                 1                    1                     1   \n",
            "\n",
            "   '21:9416257:C:A'  '21:9417127:C:A'  \n",
            "0                 0                 1  \n",
            "1                 0                 0  \n",
            "2                 1                 1  \n",
            "3                 1                 1  \n",
            "4                 1                 1  \n",
            "\n",
            "DataFrame 'genotip_matrisi_tum_snpler_chr21.csv' dosyasına kaydediliyor...\n",
            "'genotip_matrisi_tum_snpler_chr21.csv' başarıyla kaydedildi!\n"
          ]
        }
      ]
    }
  ]
}